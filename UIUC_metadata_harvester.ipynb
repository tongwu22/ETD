{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sickle\n",
      "  Downloading Sickle-0.7.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pandas in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests>=1.1.0 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from sickle) (2.31.0)\n",
      "Collecting lxml>=3.2.3 (from sickle)\n",
      "  Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from requests>=1.1.0->sickle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from requests>=1.1.0->sickle) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from requests>=1.1.0->sickle) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from requests>=1.1.0->sickle) (2023.11.17)\n",
      "Downloading Sickle-0.7.0-py3-none-any.whl (12 kB)\n",
      "Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, sickle\n",
      "Successfully installed lxml-5.3.0 sickle-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sickle pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirawu/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sickle import Sickle  # For OAI-PMH harvesting\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import time  # For implementing delays\n",
    "from requests.exceptions import RequestException  # For error handling\n",
    "from lxml import etree  # For XML parsing\n",
    "\n",
    "# Step 2: Define a function to parse each record\n",
    "def parse_record(record):\n",
    "    \"\"\"\n",
    "    Parse an OAI-PMH record and extract relevant metadata fields.\n",
    "    \n",
    "    :param record: An OAI-PMH record object\n",
    "    :return: A dictionary containing parsed metadata\n",
    "    \"\"\"\n",
    "    # Parse the XML content of the record\n",
    "    root = etree.fromstring(record.raw)\n",
    "    \n",
    "    # Define namespace\n",
    "    ns = {'oai_dc': 'http://www.openarchives.org/OAI/2.0/oai_dc/',\n",
    "          'dc': 'http://purl.org/dc/elements/1.1/'}\n",
    "    \n",
    "    # Extract metadata\n",
    "    get_text = lambda tag: '; '.join(e.text for e in root.findall(f'.//dc:{tag}', ns) if e.text)\n",
    "    \n",
    "    return {\n",
    "        'identifier': record.header.identifier,  # Unique identifier for the record\n",
    "        'datestamp': record.header.datestamp,  # Last modification date of the record\n",
    "        'title': metadata.get('title', [None])[0],  # Title of the work\n",
    "        'creator': '; '.join(metadata.get('creator', [])),  # Author(s) of the work\n",
    "        'date': '; '.join(metadata.get('date', [])),  # Relevant dates (e.g., publication, submission)\n",
    "        'description': '; '.join(metadata.get('description', [])),  # Abstract or other descriptions\n",
    "        'subject': '; '.join(metadata.get('subject', [])),  # Subject terms or keywords\n",
    "        'publisher': metadata.get('publisher', [None])[0],  # Publisher information\n",
    "        'type': '; '.join(metadata.get('type', [])),  # Type of the work (e.g., thesis, dissertation)\n",
    "        'language': metadata.get('language', [None])[0],  # Language of the work\n",
    "        'relation': '; '.join(metadata.get('relation', [])),  # Related information (e.g., report numbers)\n",
    "        'identifier_url': metadata.get('identifier', [None])[0],  # URL or DOI of the work\n",
    "    }\n",
    "\n",
    "# Step 3: Set up the OAI-PMH client\n",
    "base_url = \"https://www.ideals.illinois.edu/oai-pmh\"\n",
    "sickle = Sickle(base_url)\n",
    "\n",
    "# Step 4: Define harvesting parameters\n",
    "metadata_prefix = \"oai_dc\"  # We're using the Dublin Core metadata format\n",
    "set_spec = \"com_2142_5130\"  # Graduate Dissertations and Theses at Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the harvesting process...\n",
      "Harvested 1000 records...\n",
      "Harvested 2000 records...\n",
      "Harvested 3000 records...\n",
      "Harvested 4000 records...\n",
      "Harvested 5000 records...\n",
      "Harvested 6000 records...\n",
      "Harvested 7000 records...\n",
      "Harvested 8000 records...\n",
      "Harvested 9000 records...\n",
      "Harvested 10000 records...\n",
      "Harvested 11000 records...\n",
      "Harvested 12000 records...\n",
      "Harvested 13000 records...\n",
      "Harvested 14000 records...\n",
      "Harvested 15000 records...\n",
      "Harvested 16000 records...\n",
      "Harvested 17000 records...\n",
      "Harvested 18000 records...\n",
      "Harvested 19000 records...\n",
      "Harvested 20000 records...\n",
      "Harvested 21000 records...\n",
      "Harvested 22000 records...\n",
      "Harvested 23000 records...\n",
      "Harvested 24000 records...\n",
      "Harvested 25000 records...\n",
      "Harvested 26000 records...\n",
      "Harvested 27000 records...\n",
      "Harvested 28000 records...\n",
      "Harvested 29000 records...\n",
      "Harvested 30000 records...\n",
      "Harvested 31000 records...\n",
      "Harvested 32000 records...\n",
      "Harvested 33000 records...\n",
      "Harvested 34000 records...\n",
      "Harvested 35000 records...\n",
      "Harvested 36000 records...\n",
      "Harvested 37000 records...\n",
      "Harvested 38000 records...\n",
      "Harvested 39000 records...\n",
      "Harvested 40000 records...\n",
      "Harvested 41000 records...\n",
      "Harvested 42000 records...\n",
      "Harvested 43000 records...\n",
      "Harvested 44000 records...\n",
      "Harvested 45000 records...\n",
      "Harvested 46000 records...\n",
      "Harvested 47000 records...\n",
      "Harvested 48000 records...\n",
      "Harvested 49000 records...\n",
      "Harvested 50000 records...\n",
      "Harvested 51000 records...\n",
      "Harvested 52000 records...\n",
      "Harvested 53000 records...\n",
      "Total records harvested: 53471\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Harvest the records\n",
    "print(\"Starting the harvesting process...\")\n",
    "records = []\n",
    "retries = 3 # Number of retry attempts for each record\n",
    "for record in sickle.ListRecords(metadataPrefix=metadata_prefix, set=set_spec):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            parsed_record = parse_record(record)\n",
    "            records.append(parsed_record)\n",
    "            if len(records) % 1000 == 0:\n",
    "                print(f\"Harvested {len(records)} records...\")\n",
    "                time.sleep(1) # Sleep for 1 second every 1000 records to avoid overwhelming the server\n",
    "            break # Exit the retry loop if successful\n",
    "        except RequestException as e:\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Error occurred: {e}. Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(f\"Failed to harvest record after {retries} attempts.\")\n",
    "\n",
    "print(f\"Total records harvested: {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to uiuc_etd_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Convert the harvested records to a pandas DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Step 7: Save the data to a CSV file\n",
    "df.to_csv('uiuc_etd_metadata.csv', index=False)\n",
    "print(\"Data saved to uiuc_etd_metadata.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic analysis of the harvested data:\n",
      "Total number of records: 53471\n",
      "Date range: from  to 2024-09-16T10:20:27-05:00\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Perform basic analysis on the harvested data\n",
    "print(\"\\nBasic analysis of the harvested data:\")\n",
    "print(f\"Total number of records: {len(df)}\")\n",
    "\n",
    "# Calculate the date range, assuming the first date in the list is the most relevant\n",
    "print(f\"Date range: from {df['date'].str.split(';').str[0].min()} to {df['date'].str.split(';').str[0].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last modified date: 2023-07-10 00:00:00\n",
      "Number of records with update dates after 2023-07-10 00:00:00: 3338\n",
      "\n",
      "Sample of records with future update dates:\n",
      "                                 identifier  \\\n",
      "317  oai:www.ideals.illinois.edu:2142/97548   \n",
      "351  oai:www.ideals.illinois.edu:2142/97582   \n",
      "973  oai:www.ideals.illinois.edu:2142/98611   \n",
      "978  oai:www.ideals.illinois.edu:2142/98616   \n",
      "983  oai:www.ideals.illinois.edu:2142/98621   \n",
      "\n",
      "                                                 title earliest_date  \\\n",
      "317  Global poverty, women’s empowerment, and highe...    2017-04-14   \n",
      "351  States of discretion: Black migrating bodies a...    2017-04-18   \n",
      "973  A study of the effect of rest on the fatigue s...    1919-01-01   \n",
      "978  Highway location in the foothills of the Rocky...    1936-01-01   \n",
      "983  The viability of the nodule bacteria of legume...    1921-01-01   \n",
      "\n",
      "            update_date  \n",
      "317 2025-04-23 05:00:00  \n",
      "351 2023-11-03 05:00:00  \n",
      "973 2040-01-01 06:00:00  \n",
      "978 2032-01-01 06:00:00  \n",
      "983 2042-01-01 06:00:00  \n",
      "\n",
      "Distribution of records with future update dates by year:\n",
      "update_date\n",
      "2023    1459\n",
      "2024    1850\n",
      "2025       1\n",
      "2028       1\n",
      "2029       1\n",
      "2032       1\n",
      "2040       1\n",
      "2042       1\n",
      "2045       1\n",
      "2053       1\n",
      "2056       1\n",
      "2062       1\n",
      "2067       1\n",
      "2069       1\n",
      "2070       2\n",
      "2071       1\n",
      "2084       1\n",
      "2103      10\n",
      "2105       1\n",
      "2125       1\n",
      "2128       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of records with update date equal to last modified date: 0\n",
      "\n",
      "Distribution of earliest dates by year:\n",
      "earliest_date\n",
      "1875.0       4\n",
      "1877.0      10\n",
      "1880.0       3\n",
      "1881.0       2\n",
      "1883.0       3\n",
      "          ... \n",
      "2020.0    1170\n",
      "2021.0    1239\n",
      "2022.0    1242\n",
      "2023.0    1236\n",
      "2024.0     336\n",
      "Name: count, Length: 144, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('uiuc_etd_metadata.csv')\n",
    "\n",
    "# Function to extract the earliest and latest dates\n",
    "def extract_dates(date_string):\n",
    "    if pd.isna(date_string):\n",
    "        return pd.NaT, pd.NaT\n",
    "    dates = date_string.split(';')\n",
    "    valid_dates = []\n",
    "    for date in dates:\n",
    "        try:\n",
    "            parsed_date = pd.to_datetime(date.strip(), errors='coerce')\n",
    "            if not pd.isna(parsed_date):\n",
    "                # Convert to UTC and then remove timezone info\n",
    "                if parsed_date.tzinfo is not None:\n",
    "                    parsed_date = parsed_date.tz_convert('UTC').tz_localize(None)\n",
    "                valid_dates.append(parsed_date)\n",
    "        except:\n",
    "            continue\n",
    "    if valid_dates:\n",
    "        return min(valid_dates), max(valid_dates)\n",
    "    else:\n",
    "        return pd.NaT, pd.NaT\n",
    "\n",
    "# Apply the function to create new 'earliest_date' and 'update_date' columns\n",
    "df['earliest_date'], df['update_date'] = zip(*df['date'].apply(extract_dates))\n",
    "\n",
    "# Set the last_modified_date\n",
    "last_modified_date = pd.to_datetime(\"2023-07-10\")\n",
    "print(f\"Last modified date: {last_modified_date}\")\n",
    "\n",
    "# Identify records where 'update_date' is after last_modified_date\n",
    "future_records = df[df['update_date'] > last_modified_date]\n",
    "print(f\"Number of records with update dates after {last_modified_date}: {len(future_records)}\")\n",
    "\n",
    "if not future_records.empty:\n",
    "    print(\"\\nSample of records with future update dates:\")\n",
    "    print(future_records[['identifier', 'title', 'earliest_date', 'update_date']].head())\n",
    "    \n",
    "    # Count records by year for future update dates\n",
    "    future_years = future_records['update_date'].dt.year.value_counts().sort_index()\n",
    "    print(\"\\nDistribution of records with future update dates by year:\")\n",
    "    print(future_years)\n",
    "else:\n",
    "    print(\"No records found with update dates after 2023-07-10.\")\n",
    "\n",
    "# Additional analysis: Check for records with update_date equal to last_modified_date\n",
    "records_on_last_date = df[df['update_date'].dt.date == last_modified_date.date()]\n",
    "print(f\"\\nNumber of records with update date equal to last modified date: {len(records_on_last_date)}\")\n",
    "\n",
    "if not records_on_last_date.empty:\n",
    "    print(\"\\nSample of records with update date equal to last modified date:\")\n",
    "    print(records_on_last_date[['identifier', 'title', 'earliest_date', 'update_date']].head())\n",
    "\n",
    "# Distribution of earliest dates\n",
    "print(\"\\nDistribution of earliest dates by year:\")\n",
    "earliest_years = df['earliest_date'].dt.year.value_counts().sort_index()\n",
    "print(earliest_years) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record matching the title:\n",
      "                            identifier  datestamp                                                       title    creator                                                            date                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               description                                                       subject publisher       type language relation                   identifier_url\n",
      "oai:www.ideals.illinois.edu:2142/97273 2023-07-11 Three papers in urban and regional economic and development Yu, Chenxi 2017-08-10T19:14:33Z; 2017-08-10T19:14:33Z; 2017-04-17; 2017-05 \"This dissertation presents three chapters that studies the regional evolution over time and how the local markets adapt to the changing environment.  \\nThe first chapter focuses on the regional convergence or divergence debate. Current studies have provided conflicting evidences.  The regression analysis study (Barro & Sala-i-Martin, 1992) finds evidence supporting regional convergence theory. While the distributional dynamic study (Quah, 1996) provides evidence to support club convergence theory.  In this chapter, the finite mixture model is introduced as a new exploratory method to study the regional growth issue.  This study finds the emergence of convergence clubs in the United States since the 1980s. The finite mixture normal model is used to identify the clubs based on the per capita personal income dataset for 700 U.S. labor market areas from 1969 to 2009.  The results reveal that the collection of high income areas, termed the \"\"rich places club,\"\" was formed in the 1980s, and the share of the rich places club stabilized at around 10-12% of total labor market areas for the 1990s and 2000s.  We also find that the gap between the rich places club and the \"\"everywhere else club\"\" has been increasing since the 1990s.   \\nTo better understand what is driving the formation of the convergence clubs found in chapter one, chapter two studies how expected labor demand shifter and natural amenities impact the local market.  Traditionally, the local labor market literature focuses on price signals (wage and housing rent) and operates under a spatial equilibrium assumption, while the local economic development literature focuses on job creation, migration and operates under a spatial disequilibrium assumption.  In this chapter, a united local economy framework is presented that links the local labor market and local economic development literatures and explores four aspects of local economy: wages, housing rent, job growth, and population growth.  In the empirical section of this paper, two key factors, an expected labor demand shifter and appreciation of natural amenity, are investigated to show how they impact the four featured aspects of the local economy.   \\nFrom a local price perspective, as the expected labor demand increase, both wages and housing rents increase.  The natural amenities do not significantly impact inter-regional wage difference, but natural amenities are a significant factor for inter-regional rent levels.  From a job growth and a population growth perspective, a one unit job increase in expected labor demand growth will create more than one additional jobs (1.367-1.392).  For every one unit increase of expected labor demand shifter, population will increase 0.8.  Regions with higher amenities not only attract new population, they are also places where more jobs are created.  \\nThis chapter provides evidence that the expected labor demand shifters (ELDS) and natural amenities could significantly impact local market outcome.  Therefore, public policies can be draw up for different types of regions (type 1 high ELDS high amenities, type 2 high ELDS and low amenities, type 3 low ELDS and high amenities, type 4 low ELDS and low amenities). For type 1 regions, special attention should be paid to housing rent affordability, because both high ELDS and high natural amenities could drive up the housing rent.  For type 2 region, human capital retention could be a challenging issue.  For type 3 regions, public policy could focus on how to translate their desirable natural amenities into local, economic and social development.  And for type 4 regions, while these regions are likely going to decline, it is very important to evaluate whether public policy should focus on bringing jobs to these regions or help people move out of these regions. \\nThe “rich places club” found in chapter one are usually places with larger population.  Therefore, chapter three looks directly into the question: Why do people living in urban areas, especially large urban areas, receive higher wages?  Based on the theory on agglomeration economies, labor market matching and knowledge spillover are considered to be two of the primary micro-foundations.  Most empirical literature has found sizeable positive effects from labor market matching (Heuermann et al., 2010; Melo et al., 2009).  However, there is far less consensus on the existence of knowledge spillovers.  The reason for that is the difficulty in identifying knowledge spillover effects.  The identification challenge comes from three directions: direction of causality (Duranton, 2006), the inability to distinguish imperfect substitution from externalities (Moretti, 2004), and sorting (Wheeler, 2001).    Corresponding strategies are developed to ease the estimation biases.  This study presents three major findings: first, from 2000 to 2011, the contribution of human capital externalities to productivity growth is at least three times the contribution of the labor market matching effect; secondly, this paper finds that higher skill groups experience higher human capital externality effects; third, the human capital externalities observed for the low skill group are more likely to be a migration sorting effect. \\nThis study also finds that younger workers are more likely to benefit from the labor market matching effect while older workers are more likely to benefit from the human capital externalities effect.  Among younger workers, the group without a high school degree shows no gain from either human capital externalities or labor market matching effects.  This group of low-skilled young adults should be the central focus for human capital policy.  Meanwhile, older and highly educated workers seems to gain large benefits from both human capital externalities and labor market matching effects.  This group of workers should be encouraged to work longer (Munnell & Sass, 2009).\\n Further research should further examine the low-skilled young adult group.  Is there a strong inflow of immigrants that could be impacting this group? Are white and non-white, low-skilled younger adult labor market performance similar or different?  Is there a gender performance difference in the low-skilled young adult group? Looking deeper into these issues can help form better policy to help this segment of the labor market.\"; Submission original under an indefinite embargo labeled 'Open Access'. The submission was exported from vireo on 2017-08-10 without embargo terms; The student, Chenxi Yu, accepted the attached license on 2017-03-14 at 10:58.; The student, Chenxi Yu, submitted this Dissertation for approval on 2017-03-14 at 10:58.; This Dissertation was approved for publication on 2017-04-17 at 10:57.; DSpace SAF Submission Ingestion Package generated from Vireo submission #10590 on 2017-08-10 at 13:38:06; Made available in DSpace on 2017-08-10T19:14:33Z (GMT). No. of bitstreams: 2\\nYU-DISSERTATION-2017.pdf: 1366553 bytes, checksum: 5a9588fce9f30551c5c68e7a8f17ebb2 (MD5)\\nLICENSE.txt: 4206 bytes, checksum: a7259f54b557e47079ecd053ae628f03 (MD5)\\n  Previous issue date: 2017-04-17 Regional inequality; Labor market outcome; Wage externalities       NaN text; text       en      NaN http://hdl.handle.net/2142/97273\n",
      "\n",
      "The paper 100% match was not found in the CSV file.\n",
      "\n",
      "Matches by title: 1\n",
      "\n",
      "Matches by creator: 2\n",
      "Matches by identifier: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('uiuc_etd_metadata.csv')\n",
    "\n",
    "# Define the search criteria\n",
    "title = \"Three papers in urban and regional economic and development\"\n",
    "creator = \"Yu, Chenxi\"\n",
    "identifier = \"http://hdl.handle.net/2142/97273\"\n",
    "\n",
    "# Search for the paper\n",
    "result = df[(df['title'] == title) & \n",
    "            (df['creator'] == creator) & \n",
    "            (df['identifier'] == identifier)]\n",
    "\n",
    "# Print the matched title record\n",
    "if not result_by_title.empty:\n",
    "    print(\"Record matching the title:\")\n",
    "    print(result_by_title.to_string(index=False))\n",
    "else:\n",
    "    print(\"No record found matching the title.\")\n",
    "\n",
    "# Check if the paper was found\n",
    "if not result.empty:\n",
    "    print(\"\\nThe paper 100% match was found in the CSV file.\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"\\nThe paper 100% match was not found in the CSV file.\")\n",
    "\n",
    "# If you want to search by just one or two criteria, you can use:\n",
    "result_by_title = df[df['title'] == title]\n",
    "result_by_creator = df[df['creator'] == creator]\n",
    "result_by_identifier = df[df['identifier'] == identifier]\n",
    "\n",
    "print(f\"\\nMatches by title: {len(result_by_title)}\")\n",
    "print(f\"\\nMatches by creator: {len(result_by_creator)}\")\n",
    "print(f\"Matches by identifier: {len(result_by_identifier)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
