{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sickle\n",
      "  Downloading Sickle-0.7.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pandas in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests>=1.1.0 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from sickle) (2.31.0)\n",
      "Collecting lxml>=3.2.3 (from sickle)\n",
      "  Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from requests>=1.1.0->sickle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from requests>=1.1.0->sickle) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from requests>=1.1.0->sickle) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amirawu/Library/Python/3.9/lib/python/site-packages (from requests>=1.1.0->sickle) (2023.11.17)\n",
      "Downloading Sickle-0.7.0-py3-none-any.whl (12 kB)\n",
      "Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, sickle\n",
      "Successfully installed lxml-5.3.0 sickle-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sickle pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirawu/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sickle import Sickle  # For OAI-PMH harvesting\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import time  # For implementing delays\n",
    "from requests.exceptions import RequestException  # For error handling\n",
    "from lxml import etree  # For XML parsing\n",
    "\n",
    "# Step 2: Define a function to parse each record\n",
    "def parse_record(record):\n",
    "    \"\"\"\n",
    "    Parse an OAI-PMH record and extract relevant metadata fields.\n",
    "    \n",
    "    :param record: An OAI-PMH record object\n",
    "    :return: A dictionary containing parsed metadata\n",
    "    \"\"\"\n",
    "    # Parse the XML content of the record\n",
    "    root = etree.fromstring(record.raw)\n",
    "    \n",
    "    # Define namespace\n",
    "    ns = {'oai_dc': 'http://www.openarchives.org/OAI/2.0/oai_dc/',\n",
    "          'dc': 'http://purl.org/dc/elements/1.1/'}\n",
    "    \n",
    "    # Extract metadata\n",
    "    get_text = lambda tag: '; '.join(e.text for e in root.findall(f'.//dc:{tag}', ns) if e.text)\n",
    "    \n",
    "    return {\n",
    "        'identifier': record.header.identifier,  # Unique identifier for the record\n",
    "        'datestamp': record.header.datestamp,  # Last modification date of the record\n",
    "        'title': metadata.get('title', [None])[0],  # Title of the work\n",
    "        'creator': '; '.join(metadata.get('creator', [])),  # Author(s) of the work\n",
    "        'date': '; '.join(metadata.get('date', [])),  # Relevant dates (e.g., publication, submission)\n",
    "        'description': '; '.join(metadata.get('description', [])),  # Abstract or other descriptions\n",
    "        'subject': '; '.join(metadata.get('subject', [])),  # Subject terms or keywords\n",
    "        'publisher': metadata.get('publisher', [None])[0],  # Publisher information\n",
    "        'type': '; '.join(metadata.get('type', [])),  # Type of the work (e.g., thesis, dissertation)\n",
    "        'language': metadata.get('language', [None])[0],  # Language of the work\n",
    "        'relation': '; '.join(metadata.get('relation', [])),  # Related information (e.g., report numbers)\n",
    "        'identifier_url': metadata.get('identifier', [None])[0],  # URL or DOI of the work\n",
    "    }\n",
    "\n",
    "# Step 3: Set up the OAI-PMH client\n",
    "base_url = \"https://www.ideals.illinois.edu/oai-pmh\"\n",
    "sickle = Sickle(base_url)\n",
    "\n",
    "# Step 4: Define harvesting parameters\n",
    "metadata_prefix = \"oai_dc\"  # We're using the Dublin Core metadata format\n",
    "set_spec = \"com_2142_5130\"  # Graduate Dissertations and Theses at Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the harvesting process...\n",
      "Harvested 1000 records...\n",
      "Harvested 2000 records...\n",
      "Harvested 3000 records...\n",
      "Harvested 4000 records...\n",
      "Harvested 5000 records...\n",
      "Harvested 6000 records...\n",
      "Harvested 7000 records...\n",
      "Harvested 8000 records...\n",
      "Harvested 9000 records...\n",
      "Harvested 10000 records...\n",
      "Harvested 11000 records...\n",
      "Harvested 12000 records...\n",
      "Harvested 13000 records...\n",
      "Harvested 14000 records...\n",
      "Harvested 15000 records...\n",
      "Harvested 16000 records...\n",
      "Harvested 17000 records...\n",
      "Harvested 18000 records...\n",
      "Harvested 19000 records...\n",
      "Harvested 20000 records...\n",
      "Harvested 21000 records...\n",
      "Harvested 22000 records...\n",
      "Harvested 23000 records...\n",
      "Harvested 24000 records...\n",
      "Harvested 25000 records...\n",
      "Harvested 26000 records...\n",
      "Harvested 27000 records...\n",
      "Harvested 28000 records...\n",
      "Harvested 29000 records...\n",
      "Harvested 30000 records...\n",
      "Harvested 31000 records...\n",
      "Harvested 32000 records...\n",
      "Harvested 33000 records...\n",
      "Harvested 34000 records...\n",
      "Harvested 35000 records...\n",
      "Harvested 36000 records...\n",
      "Harvested 37000 records...\n",
      "Harvested 38000 records...\n",
      "Harvested 39000 records...\n",
      "Harvested 40000 records...\n",
      "Harvested 41000 records...\n",
      "Harvested 42000 records...\n",
      "Harvested 43000 records...\n",
      "Harvested 44000 records...\n",
      "Harvested 45000 records...\n",
      "Harvested 46000 records...\n",
      "Harvested 47000 records...\n",
      "Harvested 48000 records...\n",
      "Harvested 49000 records...\n",
      "Harvested 50000 records...\n",
      "Harvested 51000 records...\n",
      "Harvested 52000 records...\n",
      "Harvested 53000 records...\n",
      "Total records harvested: 53471\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Harvest the records\n",
    "print(\"Starting the harvesting process...\")\n",
    "records = []\n",
    "retries = 3 # Number of retry attempts for each record\n",
    "for record in sickle.ListRecords(metadataPrefix=metadata_prefix, set=set_spec):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            parsed_record = parse_record(record)\n",
    "            records.append(parsed_record)\n",
    "            if len(records) % 1000 == 0:\n",
    "                print(f\"Harvested {len(records)} records...\")\n",
    "                time.sleep(1) # Sleep for 1 second every 1000 records to avoid overwhelming the server\n",
    "            break # Exit the retry loop if successful\n",
    "        except RequestException as e:\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Error occurred: {e}. Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(f\"Failed to harvest record after {retries} attempts.\")\n",
    "\n",
    "print(f\"Total records harvested: {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to uiuc_etd_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Convert the harvested records to a pandas DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Step 7: Save the data to a CSV file\n",
    "df.to_csv('uiuc_etd_metadata.csv', index=False)\n",
    "print(\"Data saved to uiuc_etd_metadata.csv\") # Save locally \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic analysis of the harvested data:\n",
      "Total number of records: 53471\n",
      "Date range: from  to 2024-09-16T10:20:27-05:00\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Perform basic analysis on the harvested data\n",
    "print(\"\\nBasic analysis of the harvested data:\")\n",
    "print(f\"Total number of records: {len(df)}\")\n",
    "\n",
    "# Calculate the date range, assuming the first date in the list is the most relevant\n",
    "print(f\"Date range: from {df['date'].str.split(';').str[0].min()} to {df['date'].str.split(';').str[0].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records with valid dates: 48604 (90.90%)\n",
      "Date range: from 2007-12-11 01:07:40+00:00 to 2024-09-16 10:20:27-05:00\n",
      "\n",
      "Sample of records without valid dates:\n",
      "                                 identifier  \\\n",
      "132  oai:www.ideals.illinois.edu:2142/97363   \n",
      "151  oai:www.ideals.illinois.edu:2142/97382   \n",
      "322  oai:www.ideals.illinois.edu:2142/97553   \n",
      "585  oai:www.ideals.illinois.edu:2142/98092   \n",
      "647  oai:www.ideals.illinois.edu:2142/98214   \n",
      "\n",
      "                                                 title  \\\n",
      "132  Stability thresholds for signed Laplacians on ...   \n",
      "151  Unbridled: the horses of Géricault’s English S...   \n",
      "322  The α-helical conformation of polypeptides: de...   \n",
      "585                                                      \n",
      "647           Geometry and topological phase of matter   \n",
      "\n",
      "                                                  date  \n",
      "132  2017-04-19; 2017-05; 2017-08-10T19:15:07Z; 201...  \n",
      "151  2017-04-24; 2017-08-10T19:15:17Z; 2017-08-10T1...  \n",
      "322  2017-04-13; 2017-05; 2017-08-10T19:51:48Z; 201...  \n",
      "585                                                     \n",
      "647  2017-05-19; 2017-09-29T16:39:01Z; 2017-09-29T1...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/pd7j7g0j3673x8rwthf6b1dh0000gn/T/ipykernel_98631/331536485.py:21: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df['first_date'] = pd.to_datetime(df['first_date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# # Handle date analysis more robustly\n",
    "# df['first_date'] = df['date'].str.split(';').str[0]\n",
    "# df['first_date'] = pd.to_datetime(df['first_date'], errors='coerce')\n",
    "\n",
    "# # Count records with valid dates\n",
    "# valid_dates = df['first_date'].notna()\n",
    "# print(f\"Records with valid dates: {valid_dates.sum()} ({valid_dates.mean():.2%})\")\n",
    "\n",
    "# if valid_dates.any():\n",
    "#     print(f\"Date range: from {df['first_date'].min()} to {df['first_date'].max()}\")\n",
    "# else:\n",
    "#     print(\"No valid dates found in the dataset.\")\n",
    "\n",
    "# # Show records without dates\n",
    "# if not valid_dates.all():\n",
    "#     print(\"\\nSample of records without valid dates:\")\n",
    "#     print(df[~valid_dates][['identifier', 'title', 'date']].head())\n",
    "\n",
    "# Handle date analysis more robustly\n",
    "df['first_date'] = df['date'].str.split(';').str[0]\n",
    "df['first_date'] = pd.to_datetime(df['first_date'], errors='coerce') \n",
    "\n",
    "# Count records with valid dates\n",
    "valid_dates = df['first_date'].notna()\n",
    "print(f\"Records with valid dates: {valid_dates.sum()} ({valid_dates.mean():.2%})\")\n",
    "\n",
    "if valid_dates.any():\n",
    "    min_date = df['first_date'].dropna().min()\n",
    "    max_date = df['first_date'].dropna().max()\n",
    "    print(f\"Date range: from {min_date} to {max_date}\")\n",
    "else:\n",
    "    print(\"No valid dates found in the dataset.\")\n",
    "\n",
    "# Show records without dates\n",
    "if not valid_dates.all():\n",
    "    print(\"\\nSample of records without valid dates:\")\n",
    "    print(df[~valid_dates][['identifier', 'title', 'date']].head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 subjects:\n",
      "subject\n",
      "Engineering, Electronics and Electrical    1782\n",
      "Computer Science                           1192\n",
      "Mathematics                                 994\n",
      "Engineering, Civil                          964\n",
      "Chemistry, Biochemistry                     854\n",
      "Physics, Condensed Matter                   726\n",
      "Chemistry, Organic                          723\n",
      "Engineering, Mechanical                     691\n",
      "Engineering, Materials Science              661\n",
      "Education, Educational Psychology           615\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Document types:\n",
      "type\n",
      "text                        56966\n",
      "Thesis                       5238\n",
      "                             3054\n",
      "Dissertation / Thesis        2503\n",
      "dissertation/thesis            83\n",
      "Text                           12\n",
      "image                           8\n",
      "Technical Report                8\n",
      "ger                             4\n",
      "Score                           4\n",
      "other                           3\n",
      "Other                           2\n",
      "Journal (whole)                 2\n",
      "Bibliography                    2\n",
      "Disseration / Thesis            1\n",
      "Report (Grant or Annual)        1\n",
      "Book Chapter                    1\n",
      "Proposal                        1\n",
      "audio                           1\n",
      "Drawing                         1\n",
      "dataset / spreadsheet           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze the top 10 subjects\n",
    "print(\"\\nTop 10 subjects:\")\n",
    "print(df['subject'].str.split(';').explode().str.strip().value_counts().head(10))\n",
    "\n",
    "# Analyze the document types\n",
    "print(\"\\nDocument types:\")\n",
    "print(df['type'].str.split(';').explode().str.strip().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
